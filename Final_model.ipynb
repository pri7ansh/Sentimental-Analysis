{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_yelp_orig_data():\n",
        "    PATH_TO_YELP_REVIEWS = INPUT_FOLDER + '/review.json'\n",
        "\n",
        "    # read the entire file into a python array\n",
        "    with open(PATH_TO_YELP_REVIEWS, 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    # remove the trailing \"\\n\" from each line\n",
        "    data = map(lambda x: x.rstrip(), data)\n",
        "\n",
        "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
        "\n",
        "    # now, load it into pandas\n",
        "    data_df = pd.read_json(data_json_str)\n",
        "\n",
        "    data_df.head(100000).to_csv(OUTPUT_FOLDER + '/output_reviews_top.csv')\n",
        "\n",
        "load_yelp_orig_data()"
      ],
      "metadata": {
        "id": "oKVErhg18jCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_data_df = pd.read_csv(INPUT_FOLDER + 'output_reviews_top.csv')\n",
        "print(\"Columns in the original dataset:\\n\")\n",
        "print(top_data_df.columns)"
      ],
      "metadata": {
        "id": "hUn9vkgL8v8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "print(\"Number of rows per star rating:\")\n",
        "print(top_data_df['stars'].value_counts())\n",
        "\n",
        "# Function to map stars to sentiment\n",
        "def map_sentiment(stars_received):\n",
        "    if stars_received <= 2:\n",
        "        return -1\n",
        "    elif stars_received == 3:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "# Mapping stars to sentiment into three categories\n",
        "top_data_df['sentiment'] = [ map_sentiment(x) for x in top_data_df['stars']]\n",
        "# Plotting the sentiment distribution\n",
        "plt.figure()\n",
        "pd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"No. of rows in df\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LPaChDJ087CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_data(top_n = 5000):\n",
        "    top_data_df_positive = top_data_df[top_data_df['sentiment'] == 1].head(top_n)\n",
        "    top_data_df_negative = top_data_df[top_data_df['sentiment'] == -1].head(top_n)\n",
        "    top_data_df_neutral = top_data_df[top_data_df['sentiment'] == 0].head(top_n)\n",
        "    top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative, top_data_df_neutral])\n",
        "    return top_data_df_small\n",
        "\n",
        "# Function call to get the top 10000 from each sentiment\n",
        "top_data_df_small = get_top_data(top_n=10000)\n",
        "\n",
        "# After selecting top few samples of each sentiment\n",
        "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
        "print(top_data_df_small['sentiment'].value_counts())\n",
        "top_data_df_small.head(10)"
      ],
      "metadata": {
        "id": "h2vGn1V1871v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the stop words\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "print(remove_stopwords(\"Restaurant had a really good service!!\"))\n",
        "print(remove_stopwords(\"I did not like the food!!\"))\n",
        "print(remove_stopwords(\"This product is not good!!\"))"
      ],
      "metadata": {
        "id": "zMIfGpnM8_w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "# Tokenize the text column to get the new column 'tokenized_text'\n",
        "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
        "print(top_data_df_small['tokenized_text'].head(10))"
      ],
      "metadata": {
        "id": "jeEqqsOS9Fet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "# Get the stemmed_tokens\n",
        "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
        "top_data_df_small['stemmed_tokens'].head(10)"
      ],
      "metadata": {
        "id": "H8QcPSX89Jes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id', 'stemmed_tokens']], \n",
        "                                                        top_data_df_small['sentiment'], \n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
      ],
      "metadata": {
        "id": "gAGgPylD9OXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "# Skip-gram model (sg = 1)\n",
        "size = 1000\n",
        "window = 3\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "\n",
        "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_' + str(size) + '.model'\n",
        "start_time = time.time()\n",
        "stemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
        "# Train the Word2Vec Model\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "w2v_model.save(word2vec_model_file)"
      ],
      "metadata": {
        "id": "QiKWWPBM9Yxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Load the model from the model file\n",
        "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "# Unique ID of the word\n",
        "print(\"Index of the word 'action':\")\n",
        "print(sg_w2v_model.wv.vocab[\"action\"].index)\n",
        "# Total number of the words \n",
        "print(len(sg_w2v_model.wv.vocab))\n",
        "# Print the size of the word2vec vector for one word\n",
        "print(\"Length of the vector generated for a word\")\n",
        "print(len(sg_w2v_model['action']))\n",
        "# Get the mean for the vectors for an example review\n",
        "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
        "print(np.mean([sg_w2v_model[token] for token in top_data_df_small['stemmed_tokens'][0]], axis=0))"
      ],
      "metadata": {
        "id": "o4d0nUqQ9dEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the vectors for train data in following file\n",
        "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
        "with open(word2vec_filename, 'w+') as word2vec_file:\n",
        "    for index, row in X_train.iterrows():\n",
        "        model_vector = (np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(1000))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')"
      ],
      "metadata": {
        "id": "sxsCOoA-9kOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uEWXeYMf9lDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#Import the DecisionTreeeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Load from the filename\n",
        "word2vec_df = pd.read_csv(word2vec_filename)\n",
        "#Initialize the model\n",
        "clf_decision_word2vec = DecisionTreeClassifier()\n",
        "\n",
        "start_time = time.time()\n",
        "# Fit the model\n",
        "clf_decision_word2vec.fit(word2vec_df, Y_train['sentiment'])\n",
        "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))"
      ],
      "metadata": {
        "id": "W1_f6HG19oVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "test_features_word2vec = []\n",
        "for index, row in X_test.iterrows():\n",
        "    model_vector = np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)\n",
        "    if type(model_vector) is list:\n",
        "        test_features_word2vec.append(model_vector)\n",
        "    else:\n",
        "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
        "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
        "print(classification_report(Y_test['sentiment'],test_predictions_word2vec))"
      ],
      "metadata": {
        "id": "mVIsKOub9qIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6x_eUXQ9tIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U6uC8waB80DD"
      }
    }
  ]
}